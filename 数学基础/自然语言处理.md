计算机能够读懂语言的前提是:这种语言是一种可计算的物体，我们将一种语言作为输入，一种作为输出，使用NLP作为中间的桥梁，首先将中文通过一种压缩机制转码成机器能理解的数字&，然后用中间这种数字化的语言表达形式，再通过一次英文的解压&，解压出来英文作为输出语言。当你想和计算机对话，计算机在收到你的语言信息后，会翻译成它能理解的数字内容，然后使用这些数字语言，通过一些处理分析，做出行为决策，最终返回人类的语言。一来一回，形成对话，解决具体问题。

* 搜索引擎

  在一篇文章可以被搜索之前，搜索引擎将对他们进行细致的观察，因为它可不想把全部的信息，不分重点的一股脑存储下来。取而代之，它会挑选重点部分，分别对待，比如重点关注标题、时间、正文。将这些信息给予不同的权重后，接着就是下一步，将它存储起来。搜索引擎通常在搜索的时候，不会临时从全网找材料，而是将刚刚收集到的信息提前构建成索引，存储在便于快速检索的数据库中。只在自己的数据库中搜索，使我们的及时搜索更有效率。如果你的网页内容有更新，你可以选择自己主动告诉搜索引擎“我更新了内容”，或者等待它定时用蜘蛛来爬取你的更新信息。

  现在的深度学习给我们提供了另一条思路，也就是用模型从非文字的信息中提取计算机能够识别的可计算信息。 在用户用文字搜索时，将搜索的文字内容转换成深度学习能识别的数字内容，然后再和之前存储的图片、视频数字信息进行匹配，对比两种数字之间的关联性，然后找到最相近的内容。这种搜索，我们有一个专业名词叫作多模态搜索。多模态搜索并不仅限于文字搜图片视频，它还能颠倒过来，用图片搜图片，图片搜视频等，因为在深度学习看来，只要它们能被转换成统一的数字形态，我就能对比相似性。

  ![](C:\Users\陶林\Pictures\JAVA\algorithm\数学基础\image\23.png)

  为了实现在海量网页和文件中的快速搜索遍历，我们不得不使用到更加传统的方法, 而把深度学习方法放到后续更加适合的步骤中。目前比较常用的方式，类似于这里的层层筛选过滤的方式，将筛选结果用不同的方法，从海量的网页中，一层层过滤到最符合你搜索条件的结果。而在需要做大量文档过滤处理的阶段，我们就使用时间消耗相对较少的技术，最后可以把深度学习方案，放在文档量和计算量都少的地方。

  倒排索引是一种批量召回技术，它能快速在海量数据中初步召回基本符合要求的文章，在第一次拿到所有材料时，把它们通读一遍，然后构建关键词和文章的对应关系。

  当处理的是海量数据的时候，通过倒排索引找到的文章可能依然是海量。如果能有种方法对这些文章进行排序操作，再选取排名靠前的文章列表也能帮我们节省大量的时间。处理匹配排序，最有名的算法之一叫做TF-IDF。

  ![](C:\Users\陶林\Pictures\JAVA\algorithm\数学基础\image\24.png)

  假设我们搜索关键词“莫烦Python”,机器会利用词表的模式计算“莫烦Python”这个问题的TF-IDF值。然后会计算问句和每篇文章的cosine距离，这个例子中的计算过程，简单来说，就是将文章按照词的维度放到一个四维空间中，然后把问句同样也放到这个空间里，最后看空间中这个问题离哪一个文章的距离最近，越近则相似度越高。通过这样的方式呢，我们就能找到搜索问题的最佳匹配文章了。

* 词向量

  计算机之所以能看懂字里行间的感情，理解文字，处理文字，并不是因为它理解的我们普罗万象的人类语言，而是它将语言或者词汇归类到了一个正确的位置上。计算机对词语的理解，其实是计算机对空间及位置的理解。计算机看懂词语的一种技术-词向量。

  在前面的内容中提到过搜索引擎当中的文章向量化技术。每一篇文章都可以用词语出现频率来表示，颜色深的地方，数字的值越大。比如文章能被表示成这样一个向量。我们将文章按照这种向量表现形式，投射到一个空间中，他们就有了自己的位置。因为每篇文章向量中的数值不一定相同，那么在空间中也会分散开来。我们将图片或者语言进行数字化后，计算机就可以操起吃饭的家伙，来算数啦。在可以被计算的空间中，我们就可以比较每一个点之间的距离，离得越近，就可以说他们越像。

  之前我们用空间中点与点之间的直线距离可以判断词汇的相似程度，而有时候我们并不在乎距离的长短（或强度的大小），只要是词语在一个方向上，我们就认为他们是相近的，这时我们就能用cosine 相似度来测量两个向量的夹角大小。如果只想测量两个词的相似度，角度信息也足够了。但点与点的距离还透露了更多的信息，只要两个词总在一起出现，他们之间的关联性应该越强，距离应该也越近。我们想一想，如果一个词不仅出现的频率高，而且任何句子中都能出现，比如“在”，“你”，“吗”这一类的词，为了得到这些词的位置，机器需要不断计算他们之间的相关性。这个过程我们称之为机器学习或者模型训练。这些词每次训练的时候都想被拉扯到独立的空间，但是被太多不同方向的词拉来拉去，比如”在”这个字，训练“在这”的时候“在”字被拉扯到靠近“这”字的方向。训练“在家”的时候，“在”字将会更靠近“家”字，后面的训练也一样，所以“在”字因为频率太高，和很多字都能混搭，它就算是之中机器认为的“中性词”，越有区分力的词可能越远离中心地带，因为他们和其他词都不像，而越通用，在每种场景都有的词，就可能越靠近原点。这时，点与点的距离就能告诉我们词的频率性特征。

  ![](C:\Users\陶林\Pictures\JAVA\algorithm\数学基础\image\25.png)

  训练词向量有一个非常聪明的方法，我们完全不需要像监督学习那样需要人工给数据打上标签，我们可以直接在原始语料上做非监督学习，只要有各种各样的文章数据就行。训练时，我们取一小段文本，取出这些词的向量表示，比如取出除了“一”字以外的词向量，然后整合到一起，表示这些文字的整体向量，用这个整体向量预测最中间那个“一”。接下来在开始下一段文字的训练。

  将这个窗口挪动一格，用前后文预测“段”字，接着将窗口依次这样扫过所有文字，用所有的前后文预测中间词，这样计算机就能将前后文的关系搞清楚，总出现在类似的上下文中间的词关系越亲密。向量在一定程度上也越相近。除了用前后文预测中间词，我们还能换一个思路，用中间词预测前后文也行。

  除了颠倒输入输出外，其他的方法都是一样，它的假设是，在某个词的周围，应该都是和这个词有关系的词，所以当我们预测关联词的时候也就会拉近这些关联词的距离，把相近的词聚集到一起，从而得到所有的词向量。 获得了这些词向量后，我们又可以怎么用呢？

  有种用法很简单，就是直接把词向量当成词语特征输入进另一个模型里。这样就能用更丰富的词向量信息来表示一个词语ID。在这种情况中，我们说词向量是一种预训练特征。用word2vec 的方法预先训练好了词语的特征表达，然后在其他场景中拿着预训练结果直接使用。

  还有种更有趣的玩法，用词向量进行加减运算，男人减掉女人的词向量，差不多就约等于公猫减掉母猫的词向量。或者你换一个思路,玩点更有趣的。

* 句向量

  使用深度学习的计算机在理解任何事物之前，都是将这件事物转换成一种数学的表达形式。在AI从业人员看来，AI技术，都是将这些以数字记录的数据，通过AI模型将其转化成一串数字。我们可以用乘法来转移空间属性，比如使用线性代数的方法，让一个空间的向量，在另一个空间以不同形式表达。而我们现在最常见的方案就是使用神经网络做这样的复杂乘法运算。在神经网络中，我们常常将这个过程叫做 Encoding，编码。

  这个过程类似压缩的过程，将大量复杂的信息，压缩成少量经典的信息，通过这个途径找到信息的精华部分。我们待会再聊聊能用这个精华部分做些什么有趣的事情。现在我们聚焦在如何通过神经网络的方法得到这个精华部分。一个人说话是存在顺序信息的，如果将词语的顺序颠倒，我们可能会得到完全不同的信息。可见我们的模型必须要将顺序信息完全考虑起来。恰好，我们之前提过，循环神经网络最擅长做这件事情。循环神经网络在词向量上，从前到后，一个个词语阅读，阅读完整句后，模型拥有了对这句话整体的理解，也就有能力产生出一个基于整句理解的向量，encoding句向量的过程也顺理成章地完成了。那么有了这个句子的理解，我们又能干嘛呢？这时就是开脑洞的时刻了。因为只要我们能够向量化的事物，都可以在这个过程中应用起来。与encoding对应的，我们还有一个叫decoding的过程。如果说encoding是编码，是压缩，那么decoding就是解码，是解压。与传统的压缩解压不同，我们不仅能解压出原始的文件，我们还能基于这种计算机压缩过的理解，转化成更多数字化形式。

  简而言之，Encoder负责理解上文，Decoder负责将思考怎么样在理解的句子的基础上做任务。这一套方法就是在自然语言处理中风靡一时的Seq2Seq框架。不得不说，分工合作的这套机制，在深度学习中十分普遍且有效。

* 注意力

  如果说在视觉上，机器可以注意到某一个区域，那么在语言上，就是注意到某一个或多个词汇。 如果我们的任务不同，这些注意力可能会想去获取不同区域的词汇。首先模型得先通读一下这段文字，毕竟如果没有上下文的信息， 我们也不知道究竟要注意些什么。通读完之后，我们可以得到一个对于这句话的理解，熟悉AI的人应该知道这东西叫句向量。 单独靠一个句向量我们实际上是通过全局信息来生成对话，那么注意力是局部信息，我们可以将全局信息配合局部信息一起来生成每一个回复的词。 

  如果深刻理解是通过注意力产生的，那么肯定也不只使用了一次注意力。这种思路正是目前AI技术发展的方向之一，利用注意力产生理解，而且使用的也是多次注意力的转换。我们之前提到的模型，在通读语言后产生一个对句子全局的理解（句向量），然后再分别将 全局理解 和部分被注意的 局部理解 效应叠加，作为我后续任务的基础，比如基于全局和局部生成回复信息。但是这并不是我们刚刚提到的在注意力上再注意。所以聪明的科研人员创造了另一种方法，他们说，根本没有什么全局理解，我们用一次一次的注意力产生的局部理解就能解决这个问题。模型也可以经过几次注意，不同层级的注意力带来的是不同层级上的理解。越是后面的注意，就是越深度的思考。如果熟悉自然语言模型的同学此刻应该也想到了，这就是 Transformer 模型。它使用的是一个个注意力矩阵来表示在不同位置的注意力强度。通过控制强度来控制信息通道的阀门大小。即使这样，研究者还觉得不够。假如这种情况是一个人看多次，我们何不尝试让多个人一起看多次呢？这样会不会更有效率，变成三个臭皮匠赛过诸葛亮？结果就有了这样的形态，多个人同时观察一句话，分别按自己的意见提出该注意哪里， 然后再汇总自己通过自己的注意力得到的结论，再进入下一轮注意力。 研究表明，这种注意力方案的确可以带给我们更深层的句意理解。